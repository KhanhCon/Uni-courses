{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from math import log\n",
    "import math, heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization and readfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    DELIM = '[ \\n\\t0123456789;:.,/\\(\\)\\\"\\'-]+'\n",
    "    return re.split(DELIM, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path, docid):\n",
    "    files = sorted(os.listdir(path))\n",
    "    f = open(os.path.join(path, files[docid]), 'r', encoding='latin-1')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document frequency of terms are stored in a dictionary. I've made used of the indextextfiles_BR() function with a little tweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "docFrequencies = {}\n",
    "for word in docFrequencies:\n",
    "    docFrequencies[word] = len(docFrequencies[word])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through whole collection of documents to get document frequency of words, then we put then in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequencies(path):\n",
    "    N = len(sorted(os.listdir(path)))\n",
    "    docFrequencies = {}\n",
    "    for docID in range(N):\n",
    "        s = readfile(path, docID)\n",
    "        words = tokenize(s)\n",
    "        for w in words:\n",
    "            if w != '':\n",
    "                if w not in docFrequencies:\n",
    "                    docFrequencies[w] = {docID}\n",
    "                else:\n",
    "                    docFrequencies[w].add(docID)\n",
    "\n",
    "    for word in docFrequencies:\n",
    "        docFrequencies[word] = len(docFrequencies[word])\n",
    "\n",
    "    return docFrequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to process the entire directory and generate the complete docfrequency dictionary we can execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentFrequencies = document_frequencies('docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get how many document does the word <code>'ferguson'</code> appears in, we execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(documentFrequencies['ferguson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked retrieval - indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorise a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present a document as a vector, I've used a dictionary where the value-key pair is term-term_frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_doctf(doc):\n",
    "    words = tokenize(doc)\n",
    "    vectorised_doc = {}\n",
    "    for w in words:\n",
    "        if w != '':\n",
    "            if w not in vectorised_doc:\n",
    "                vectorised_doc[w] = 1.0\n",
    "            else:\n",
    "                vectorised_doc[w] += 1.0\n",
    "    return vectorised_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To present <code>document 101</code> as a vector we can execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc101 = readfile('docs', 101)\n",
    "vectorised_doc = vectorise_doctf(doc101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this datastructure to find out the termfrequency of the word <code>'the'</code>  in this document. We just need to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n"
     ]
    }
   ],
   "source": [
    "print(vectorised_doc['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the magnitude of the vector, we take the sum of the square of each component then take the square root of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = math.sqrt(sum(map(lambda x: x * x, vectorised_doc.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_vector(vector):\n",
    "    coef = math.sqrt(sum(map(lambda x: x * x, vector.values())))\n",
    "    for term in vector:\n",
    "        vector[term] = vector[term] / coef\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalise a vector, we pass the vector to the function as an agrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisedDoc = normalise_vector(vectorised_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight of the word <code>'the'</code> in the normalised document would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4599069394901954\n"
     ]
    }
   ],
   "source": [
    "print(normalisedDoc['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through the directory to normalise all the document then put them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indextextfiles_RR(path, docFrequencies):\n",
    "    N = len(sorted(os.listdir(path)))\n",
    "    normalised_documents = {}\n",
    "    for docID in range(N):#\n",
    "        doc = readfile(path, docID)\n",
    "        vectorised_doc = vectorise_doctf(doc)\n",
    "        normalised_documents[docID] = normalise_vector(vectorised_doc)\n",
    "    return normalised_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to process the entire directory and generate a dictionary of normalised docuemnts, we execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisedDocuments = indextextfiles_RR('docs', documentFrequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the normalised weight of the word <code>'the'</code> in document ID <code>101</code> is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45990693949019623\n"
     ]
    }
   ],
   "source": [
    "print(normalised_documents[101][\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorise the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we make use of the <code>vectorise_doctf()</code> to vectorise the query. Then we go over the vector to recalculate with the IDF formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_query(query, docFrequencies, numberOfDocuments):\n",
    "    words = tokenize(query)\n",
    "    vectorQueryIDF = vectorise_doctf(query)\n",
    "    for term in vectorQueryIDF:\n",
    "        if term in docFrequencies:\n",
    "            vectorQueryIDF[term] = vectorQueryIDF[term]*log(numberOfDocuments/docFrequencies[term], 10)\n",
    "        else:\n",
    "            vectorQueryIDF[term] = 0.0\n",
    "    return vectorQueryIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "turn the query <code>'England played very well'</code> into a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisedQuery = vectorise_query('England played very well', documentFrequencies, len(normalisedDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the weight of the word <code>'played'</code> in the query is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5390878844203136\n"
     ]
    }
   ],
   "source": [
    "print(vectorisedQuery[\"played\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QueryRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With query <code>'England played very well'</code> and document ID <code>47</code>, we can calculate their <code>cosine similarity</code> by calculate their <code>dot product</code> since our documents are normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18237371998272772\n"
     ]
    }
   ],
   "source": [
    "docID = 47\n",
    "dotProduct = 0\n",
    "for term in vectorisedQuery:\n",
    "    if term in normalisedDocuments[docID]:\n",
    "        dotProduct += vectorised_query[term] * normalisedDocuments[docID][term]\n",
    "print(dotProduct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through all the documents they calculate the cosine similarity between each document and the query, then return the top 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_RR(query, normalisedDocs, documentFrequencies):\n",
    "    vectorised_query = vectorise_query(query, documentFrequencies, len(normalisedDocs))\n",
    "    rank = []\n",
    "    for docID in normalisedDocs:\n",
    "        dotProduct = 0\n",
    "        for term in vectorised_query:\n",
    "            if term in normalisedDocs[docID]:\n",
    "                dotProduct += vectorised_query[term] * normalisedDocs[docID][term]\n",
    "\n",
    "        rank.append((dotProduct, docID))\n",
    "    return [x[1] for x in heapq.nlargest(10, rank)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[320, 47, 722, 576, 538, 448, 533, 425, 623, 502]\n",
      "[34, 429, 530, 586, 404, 77, 551, 441, 422, 523]\n"
     ]
    }
   ],
   "source": [
    "print(query_RR('England played very well', normalisedDocuments, documentFrequencies))\n",
    "print(query_RR('federer australian wimbledon', normalisedDocuments, documentFrequencies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
